{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is me trying to integrate all of the things I've learned: 1. SVM, SVC, torch, torch, pandas, numpy\n",
    "\n",
    "The objective is to build a SVM algorithm that takes the data set un compute using SVM kernel trick to find the best hyperplain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and Reading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import file_operations\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\ImportanFiles\\Coding Related\\Repositories\\Machine Learning project related\\Project 6 SVM\n",
      "WARNING: The repository 'Machine-Learning-Project-related' was not found in the current directory path (d:\\ImportanFiles\\Coding Related\\Repositories\\Machine Learning project related\\Project 6 SVM).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m paths \u001b[38;5;241m=\u001b[39m file_operations\u001b[38;5;241m.\u001b[39mcreate_project_path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProject 6 SVM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbreast-cancer-wisconsin.data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, file_name)\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m      7\u001b[0m display(df)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "paths = file_operations.create_project_path(\"Project 6 SVM\")\n",
    "file_name = 'breast-cancer-wisconsin.data'\n",
    "file_path = os.path.join(paths[\"data_dir\"], file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "display(df)\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('?', np.nan, inplace = True)\n",
    "df.dropna(inplace = True)\n",
    "df.drop(['id'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, svm\n",
    "X = np.array(df.drop(['class'], axis = 1)).astype('float64')\n",
    "y = np.array(df['class']).astype('float64')\n",
    "# Change label from 2, 4 to -1, 1, with 1 meaning that the label is malignant -1 meaning that the label is benign.\n",
    "y = np.where(y == 4, 1, np.where(y == 2, -1, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the scikit learn version of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sk_accuracy_sum = 0\n",
    "count = 0\n",
    "for count in range(0, 10000):\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)\n",
    "    clf = svm.SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "    sk_accuracy_sum += accuracy\n",
    "    count += 1\n",
    "\n",
    "print(count)\n",
    "print(sk_accuracy_sum / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating My Own GPU accelerated version(Binary Classification):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thoughts:\n",
    "1. Checking the dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(X))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The feature is 9 dimensional, so the hyper plain best seperating hyperplain should be 8 dimensional.\n",
    "3. transfer numpy array into tensor stored on gpu.\n",
    "4. Create a randomly shuffle function to shuffle the rows but not columns of X.\n",
    "5. Create a SVC weights tensor for later train to train on. \n",
    "    The objective is to find the best weights, because SVC in this case is a normal equation of 8 dimensions, we need to find the coefficients for this equation. when found, the equation is the best seperating huperplain.\n",
    "6. Randomize weights for the SVC\n",
    "7. A function to find out a single n dimentional point to a (n-1) dimentional span's shortest vector, then multiply that vector with a classifier vector to determine wheather or not that they are reversed vectors with each other.\n",
    "    This will help determine wheather a dot is on the best seperating heperplain's \"positive side\" or \"negative side\".\n",
    "\n",
    "    Note that the first vector will be considered the \"class 1\" vector, meaning that it's relation to the best seperating hyperplain will be considered \"Class 1\"\n",
    "\n",
    "    For example:\n",
    "        if a dot is [1, 2], the line best seperating hyperplain is y = x, then the shortest vector from the dot to the hyper plain is [1.5, 1.5] - [1, 2] = [0.5, -0.5]\n",
    "        the the direction of this vector [0.5, -0.5] will be considered \"class 1\". The vectors opposed to this vector will be considered \"class 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tranfer numpy into tensor stored on gpu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Current device: {device.capitalize()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gpu = torch.tensor(X, device = device)\n",
    "y_gpu = torch.tensor(y, device = device)\n",
    "print(X_gpu.shape)\n",
    "print(y_gpu.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Shuffle the tensors' row vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randperm(X_gpu.shape[0])\n",
    "X_gpu = X_gpu[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find soft margin and SVC between each features.(Handling outliers and misclassification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations for the SVC:\n",
    "\n",
    "`Ax_1 + Bx_2 + Cx_3 + ... + Hx_8 + b = 0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`W = [A, B, C, D, E, F, G, H]`\n",
    "\n",
    "`X = [x_1, x_2, x_3, x_4, x_5, x_6, x_7, x_8]`\n",
    "\n",
    "`b` is a constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape)\n",
    "# (683, 9)\n",
    "# Randomizing a weight matrix to adjust.\n",
    "SVC_weights = torch.rand(1, X.shape[1] - 1, dtype = torch.float64, device = device)\n",
    "SVC_bias = torch.tensor(0.0, dtype=torch.float64, device=device)\n",
    "print(SVC_weights.shape)\n",
    "print(SVC_weights)\n",
    "print(SVC_weights.dtype)\n",
    "print(SVC_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to culculate the n dimensional direction vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through each point carefully with intuitive, geometric explanations.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. What Does \\( \\omega^T x_0 + b \\) Mean Geometrically?\n",
    "\n",
    "Even though \\( x_0 \\) is generally not on the hyperplane, plugging it into the expression  \n",
    "\\[\n",
    "\\omega^T x_0 + b\n",
    "\\]\n",
    "tells us something very important:\n",
    "\n",
    "- **Projection on the Normal:**  \n",
    "  Think of \\( \\omega \\) as a vector perpendicular (normal) to the hyperplane defined by  \n",
    "  \\[\n",
    "  \\omega^T x + b = 0.\n",
    "  \\]\n",
    "  When you compute \\( \\omega^T x_0 \\), you are essentially projecting the point \\( x_0 \\) onto the direction of \\( \\omega \\).\n",
    "\n",
    "- **Offset by \\( b \\):**  \n",
    "  The term \\( b \\) shifts the hyperplane away from the origin. So \\( \\omega^T x_0 + b \\) measures how far along the direction of \\( \\omega \\) the point \\( x_0 \\) lies relative to where the hyperplane is located.\n",
    "\n",
    "- **Signed Distance (up to normalization):**  \n",
    "  Although \\( x_0 \\) isn’t on the hyperplane, the sign of \\( \\omega^T x_0 + b \\) tells you on which side of the hyperplane \\( x_0 \\) lies:\n",
    "  - A positive value means it lies on the side of the hyperplane where the normal vector \\( \\omega \\) points.\n",
    "  - A negative value means it lies on the opposite side.\n",
    "  \n",
    "  The actual perpendicular (or shortest) distance from \\( x_0 \\) to the hyperplane is given by:\n",
    "  \\[\n",
    "  d = \\frac{\\omega^T x_0 + b}{\\|\\omega\\|}.\n",
    "  \\]\n",
    "  Here, \\( d \\) is a scalar that indicates both the distance and the side of the hyperplane.\n",
    "\n",
    "**Visualizing in 2D:**  \n",
    "Imagine a line in the plane representing your hyperplane. The vector \\( \\omega \\) is an arrow perpendicular to that line. Now, take any point \\( x_0 \\) in the plane. If you drop a perpendicular from \\( x_0 \\) to the line, the length of that perpendicular (and its sign) is determined by \\( \\omega^T x_0 + b \\) once you normalize it by \\( \\|\\omega\\| \\).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Your Proposed Algorithm for Classification\n",
    "\n",
    "Your understanding is on the right track. Here’s the step-by-step geometric view:\n",
    "\n",
    "1. **Random Initialization:**  \n",
    "   - **Randomize a Normal Vector \\( \\omega \\) and Bias \\( b \\).**  \n",
    "     These define your initial hyperplane. \\( \\omega \\) points perpendicular to your hyperplane, and \\( b \\) shifts the hyperplane relative to the origin.\n",
    "\n",
    "2. **Calculate the Signed Distance \\( d \\):**  \n",
    "   - For each feature vector (each training example) \\( x_0 \\), compute:\n",
    "     \\[\n",
    "     d = \\frac{\\omega^T x_0 + b}{\\|\\omega\\|}.\n",
    "     \\]\n",
    "   - **Interpretation:**\n",
    "     - If \\( d > 0 \\), the point is on one side of the hyperplane (say, Class 1).\n",
    "     - If \\( d < 0 \\), it’s on the opposite side (say, Class 2).\n",
    "\n",
    "3. **Classification Decision:**  \n",
    "   - Simply check the sign of \\( d \\). This is equivalent to checking the sign of \\( \\omega^T x_0 + b \\) (if you ignore the normalization for classification purposes).\n",
    "\n",
    "This procedure captures the essence of how SVMs use geometry to separate classes.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Defining a Loss Function and Optimizing \\( \\omega \\) and \\( b \\)\n",
    "\n",
    "Once you have the function to compute \\( d \\) for each point, you need a way to adjust \\( \\omega \\) and \\( b \\) so that the hyperplane best separates your data. This is where a loss function comes in. A popular choice is the **hinge loss**, which for a single sample is given by:\n",
    "\\[\n",
    "L_i = \\max\\{0, 1 - y_i (\\omega^T x_i + b)\\},\n",
    "\\]\n",
    "where \\( y_i \\) is the label, typically \\( +1 \\) or \\( -1 \\).\n",
    "\n",
    "#### **Intuitive Explanation in 2D**\n",
    "\n",
    "Imagine you have a set of points in the plane with labels:\n",
    "- **Correct Classification with a Margin:**  \n",
    "  For a correctly classified point (say, with \\( y_i = +1 \\)), you want:\n",
    "  \\[\n",
    "  \\omega^T x_i + b \\ge 1.\n",
    "  \\]\n",
    "  Similarly, for \\( y_i = -1 \\), you want:\n",
    "  \\[\n",
    "  \\omega^T x_i + b \\le -1.\n",
    "  \\]\n",
    "  The value “1” here defines a margin around the hyperplane.\n",
    "\n",
    "- **Misclassified Points or Points within the Margin:**  \n",
    "  If a point falls on the wrong side or within this margin, the hinge loss becomes positive, meaning you incur a penalty. This penalty encourages the model to adjust \\( \\omega \\) and \\( b \\) to push these points further into the correct region.\n",
    "\n",
    "#### **Using Gradient Descent**\n",
    "\n",
    "- **Aggregate the Loss:**  \n",
    "  Sum the hinge loss over all training examples (often also adding a regularization term like \\( \\frac{1}{2} \\|\\omega\\|^2 \\) to encourage a large margin):\n",
    "  \\[\n",
    "  L = \\frac{1}{2}\\|\\omega\\|^2 + C \\sum_{i=1}^{N} \\max\\{0, 1 - y_i (\\omega^T x_i + b)\\},\n",
    "  \\]\n",
    "  where \\( C \\) is a hyperparameter controlling the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "- **Gradient Descent Steps:**  \n",
    "  1. **Compute the Gradient:**  \n",
    "     For each misclassified or margin-violating point (i.e., where \\( 1 - y_i (\\omega^T x_i + b) > 0 \\)), calculate the gradient of the loss with respect to \\( \\omega \\) and \\( b \\).\n",
    "     \n",
    "     - The gradient with respect to \\( \\omega \\) for a single point (ignoring regularization) is:\n",
    "       \\[\n",
    "       -y_i x_i \\quad \\text{(if the point contributes to the loss)}.\n",
    "       \\]\n",
    "     - Similarly, the gradient with respect to \\( b \\) is:\n",
    "       \\[\n",
    "       -y_i.\n",
    "       \\]\n",
    "     - Add the derivative of the regularization term \\( \\omega \\) (from \\( \\frac{1}{2}\\|\\omega\\|^2 \\)) when updating \\( \\omega \\).\n",
    "\n",
    "  2. **Update Parameters:**  \n",
    "     Use gradient descent updates:\n",
    "     \\[\n",
    "     \\omega \\leftarrow \\omega - \\eta \\cdot \\text{(gradient w.r.t. } \\omega\\text{)},\n",
    "     \\]\n",
    "     \\[\n",
    "     b \\leftarrow b - \\eta \\cdot \\text{(gradient w.r.t. } b\\text{)},\n",
    "     \\]\n",
    "     where \\( \\eta \\) is the learning rate.\n",
    "\n",
    "- **Geometric Interpretation in 2D:**  \n",
    "  Picture your hyperplane (a line in 2D). For every point that is either misclassified or too close to the line (inside the margin), the loss function \"pushes\" the hyperplane away from that point:\n",
    "  - If a positive point is too close or misclassified, the gradient will adjust the line so that the distance (as measured by \\( \\omega^T x_i + b \\)) increases.\n",
    "  - If a negative point is in the wrong region, the update will adjust the line in the opposite direction.\n",
    "  \n",
    "  As you iterate, the hyperplane shifts and rotates until the margin is maximized (points are well-separated) and misclassifications are minimized.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **\\( \\omega^T x_0 + b \\)** is the projection of the point \\( x_0 \\) onto the normal direction \\( \\omega \\), shifted by \\( b \\). It tells you the signed distance (up to normalization) from \\( x_0 \\) to the hyperplane.\n",
    "2. **Your Algorithm:**  \n",
    "   - Randomize \\( \\omega \\) and \\( b \\).\n",
    "   - For each point, compute the signed distance \\( d = \\frac{\\omega^T x_0 + b}{\\|\\omega\\|} \\).\n",
    "   - Classify based on the sign of \\( d \\) (or equivalently, \\( \\omega^T x_0 + b \\)).\n",
    "3. **Loss Function and Optimization:**  \n",
    "   - Use a loss function (like hinge loss) that penalizes misclassified points and those within the margin.\n",
    "   - Sum the loss over your training set, add a regularization term, and use gradient descent to update \\( \\omega \\) and \\( b \\).\n",
    "   - In 2D, imagine adjusting a line so that all points are on the correct side and as far from the line as possible, which is exactly what the optimization process achieves.\n",
    "\n",
    "This geometric perspective should help you visualize and implement the SVM training process using gradient descent in your PyTorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_signed_distance(n_point, hyperplain_weights, hyperplain_bias):    \n",
    "    # Check input valitility function HERE:\n",
    "    \"\"\"\n",
    "    Compute the signed distance from each data point in X to the hyperplane defined by weights and bias.\n",
    "    \n",
    "    Parameters:\n",
    "      X(n_point) (Tensor): A tensor of shape (n_samples, n_features) containing the feature vectors.\n",
    "      hyperplain_weights (Tensor): A tensor of shape (1, n_features) representing the hyperplane's normal vector.\n",
    "      hyperplain_bias (Tensor): A scalar tensor representing the hyperplane's bias.\n",
    "    \n",
    "    Returns:\n",
    "      distances (Tensor): A tensor of shape (n_samples, 1) with the signed distances.\n",
    "                          Positive means the point is on the side of the hyperplane pointed to by weights; negative means the opposite.\n",
    "    \"\"\"\n",
    "    # Calculate the raw score for each sample:\n",
    "    #   For each sample x: raw_score = weights * x^T + bias\n",
    "    # This yields a tensor of shape (n_samples, 1).\n",
    "    raw_scores = torch.matmul(n_point, hyperplain_weights.T) + hyperplain_bias\n",
    "    \n",
    "    # Compute the norm (magnitude) of the weights vector.\n",
    "    weight_norm = torch.norm(hyperplain_weights)\n",
    "    \n",
    "    # Calculate the signed distance: divide the raw score by the norm.\n",
    "    # This gives the perpendicular distance from the point to the hyperplane.\n",
    "    distances = raw_scores / weight_norm\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hinge_loss(distances, labels, margin=1.0):\n",
    "    \"\"\"\n",
    "    Compute the mean hinge loss for a set of predictions.\n",
    "    \n",
    "    Parameters:\n",
    "      distances (Tensor): A tensor of shape (n_samples, 1) with signed distances from the hyperplane.\n",
    "      labels (Tensor): A tensor of shape (n_samples,) with the true labels (+1 or -1).\n",
    "      margin (float): The margin value used in the hinge loss (default is 1.0).\n",
    "    \n",
    "    Returns:\n",
    "      loss (Tensor): A scalar tensor representing the mean hinge loss over the batch.\n",
    "    \"\"\"\n",
    "    # Remove the extra dimension for convenience.\n",
    "    distances = distances.squeeze()\n",
    "    \n",
    "    # Compute the hinge loss for each sample:\n",
    "    #   loss_i = max(0, margin - label_i * distance_i)\n",
    "    losses = torch.clamp(margin - labels * distances, min=0)\n",
    "    \n",
    "    # Return the average loss.\n",
    "    return losses.mean()\n",
    "\n",
    "def update_model(X, y, weights, bias, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Perform one gradient descent update step for the SVM parameters.\n",
    "    \n",
    "    Parameters:\n",
    "      X (Tensor): A tensor of shape (n_samples, n_features) with the input data.\n",
    "      y (Tensor): A tensor of shape (n_samples,) with the true labels.\n",
    "      weights (Tensor): A tensor of shape (1, n_features) representing the current weights.\n",
    "      bias (Tensor): A scalar tensor representing the current bias.\n",
    "      learning_rate (float): The learning rate for gradient descent.\n",
    "    \n",
    "    Returns:\n",
    "      loss_value (float): The computed loss value for this batch.\n",
    "      weights (Tensor): The updated weights.\n",
    "      bias (Tensor): The updated bias.\n",
    "    \"\"\"\n",
    "    # Enable gradient tracking on weights and bias.\n",
    "    weights.requires_grad_(True)\n",
    "    bias.requires_grad_(True)``\n",
    "    \n",
    "    # Compute the signed distances for all training samples.\n",
    "    distances = cal_signed_distance(X, weights, bias)\n",
    "    \n",
    "    # Compute the hinge loss given the distances and true labels.\n",
    "    loss = hinge_loss(distances, y)\n",
    "    \n",
    "    # Perform backpropagation to compute the gradients.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights and bias using gradient descent.\n",
    "    with torch.no_grad():\n",
    "        weights -= learning_rate * weights.grad\n",
    "        bias -= learning_rate * bias.grad\n",
    "    \n",
    "    # Zero out the gradients after the update.\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()\n",
    "    \n",
    "    return loss.item(), weights, bias\n",
    "\n",
    "# ----- Example Training Loop -----\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_value, SVC_weights, SVC_bias = update_model(X_gpu, y_gpu, SVC_weights, SVC_bias, learning_rate)\n",
    "    print(f\"Epoch {epoch + 1:03d} | Loss: {loss_value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
